|    |   seq_len |   d_model | forward_ms_mean   | forward_ms_std   | backward_ms_mean   | backward_ms_std   | fwd_peak_mem_MB   |   batch_size |
|---:|----------:|----------:|:------------------|:-----------------|:-------------------|:------------------|:------------------|-------------:|
|  0 |       256 |        16 | 0.326             | 0.122            | 0.751              | 0.236             | 20.9              |            8 |
|  1 |      1024 |        16 | 0.459             | 0.057            | 1.116              | 0.143             | 82.8              |            8 |
|  2 |      4096 |        16 | 4.34              | 0.025            | 10.738             | 0.075             | 1050.6            |            8 |
|  3 |      8192 |        16 | 16.88             | 0.029            | 42.042             | 0.116             | 4133.0            |            8 |
|  4 |     16384 |        16 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
|  5 |       256 |        32 | 0.201             | 0.016            | 1.056              | 0.025             | 21.5              |            8 |
|  6 |      1024 |        32 | 0.422             | 0.065            | 1.139              | 0.184             | 85.3              |            8 |
|  7 |      4096 |        32 | 4.391             | 0.029            | 10.806             | 0.085             | 1060.6            |            8 |
|  8 |      8192 |        32 | 17.151            | 0.022            | 42.248             | 0.089             | 4153.0            |            8 |
|  9 |     16384 |        32 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
| 10 |       256 |        64 | 0.215             | 0.028            | 1.048              | 0.039             | 22.8              |            8 |
| 11 |      1024 |        64 | 0.428             | 0.054            | 1.176              | 0.157             | 90.3              |            8 |
| 12 |      4096 |        64 | 4.561             | 0.058            | 11.123             | 0.045             | 1080.6            |            8 |
| 13 |      8192 |        64 | 17.808            | 0.019            | 43.221             | 0.091             | 4193.0            |            8 |
| 14 |     16384 |        64 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
| 15 |       256 |       128 | 0.185             | 0.013            | 1.066              | 0.096             | 25.3              |            8 |
| 16 |      1024 |       128 | 0.457             | 0.057            | 1.128              | 0.14              | 100.3             |            8 |
| 17 |      4096 |       128 | 5.535             | 0.032            | 12.904             | 0.081             | 1120.6            |            8 |
| 18 |      8192 |       128 | 21.8              | 0.025            | 50.347             | 0.126             | 4273.0            |            8 |
| 19 |     16384 |       128 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |

## 1. 显存占用推导 (Memory Accounting)

我们选择表格中**最小的 OOM 配置**：`batch_size=8`, `d_model=16`, `seq_len=16384`。

### 核心开销：注意力评分矩阵 (Attention Score Matrix)

在朴素实现中，最占内存的是 $Q K^T$ 产生的中间矩阵 $S$。

- **矩阵形状**：$(Batch, Seq, Seq) = 8 \times 16384 \times 16384$。

- **元素数量**：$8 \times 268,435,456 = 2,147,483,648$ 个元素。

- **显存占用 (float32)**：每个元素 4 bytes。

  $$2,147,483,648 \times 4 \text{ bytes} \div 1024^3 \approx 8.0 \text{ GB}$$

### 综合计算：

除了评分矩阵，还有：

- **Q, K, V 矩阵**：$3 \times (8 \times 16384 \times 16)$。
- **Softmax 输出**：同样是 $8 \times 16384 \times 16384$ (又一个 8GB)。
- **反向传播梯度**：通常需要保留前向传播的激活值。

**结论**：单单一个评分矩阵和它的 Softmax 结果就吃掉了至少 **16GB** 显存。这解释了为什么在 16384 规模下，普通的消费级 GPU 或分配的算力资源会立即报 OOM。

------

## 2. 交付回答 

根据实验数据，显存溢出（OOM）始终发生在序列长度达到 16384 时。通过内存核算可以发现，内存瓶颈主要源于存储形状为 $(Batch, Seq, Seq)$ 的注意力评分矩阵。在 `seq_len=16384` 且 `batch_size=8` 的配置下，仅这一个中间矩阵在 float32 精度下就会占据约 8GB 的显存，且为了反向传播通常还需保存经过 Softmax 后的概率矩阵，这使得显存需求随序列长度呈二次方 ($O(N^2)$) 增长。随着序列长度从 4096 翻倍至 8192，前向传播的峰值内存从约 1GB 激增至约 4GB，这种增长趋势在处理长文本任务时是不可持续的。

为了消除这一内存成本，可以采用 **FlashAttention-2** 算法。该算法利用了**算子融合（Operator Fusion）**和**分块计算（Tiling）**技术。它不再一次性显式生成并存储整个巨大的 $N \times N$ 注意力评分矩阵，而是通过 **Online Softmax** 在 SRAM 中分块计算注意力输出。这不仅将空间复杂度从 $O(N^2)$ 降低到了 $O(N)$（仅需存储最终输出和少量统计量），还通过减少对高带宽内存（HBM）的读写操作，显著提升了计算速度。