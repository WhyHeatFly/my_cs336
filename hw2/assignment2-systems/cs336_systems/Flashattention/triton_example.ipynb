{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf63e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from triton import cdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit  # 修饰器，告诉python解释器把它编译成 GPU 机器码，并在 GPU 上运行。\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,      # 输入指针\n",
    "    output_ptr,             # 输出指针\n",
    "    x_stride_row, x_stride_dim,      # 步长告诉我们如何在张量的每个轴上移动一个元素\n",
    "    weight_stride_dim,\n",
    "    output_stride_row,\n",
    "    ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,   # 分块形状必须在编译时已知\n",
    "):\n",
    "    # 每个实例将计算 x 的一个行分块的加权和\n",
    "    # 'tl.program_id' 给出当前正在运行的 “程序实例 (Program Instance)” 的 ID\n",
    "    # 当前这个程序实例负责处理输入矩阵的第几个行分块 (Row Tile)\n",
    "    row_tile_idx = tl.program_id(0)    # 0 指的网格的第0维 (即x轴) \n",
    "\n",
    "\n",
    "    # 块指针 (Block pointers) 为我们提供一种从内存的 ND (N维) 区域中进行选择\n",
    "    # 并移动我们选择区域的方法.\n",
    "    # 块指针必须知道的参数:\n",
    "    # - 指向张量第一个元素的指针\n",
    "    # - 张量的整体形状, 以处理越界访问\n",
    "    # - 每个维度的步长，以正确使用内存布局\n",
    "    # - 其实块的 ND 坐标, 即\"偏移量 (offset) \"\n",
    "    # - (block shape) 当前这个 Kernel 实例一次性要加载到芯片缓存（SRAM）里处理的一小块区域的大小\n",
    "    # - 内存中维度的顺序，从主序到次序\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D,),\n",
    "        strides=(x_stride_row, x_stride_dim),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE), \n",
    "        order=(1,0), # order 参数要求传入一个元组，代表“按步长（Stride）从小到大排序的维度索引”\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides=(weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    output_block_ptr = tl.make_block_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides=(output_stride_row,),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),  # 输出时候的偏移量 确定当前线程output应该放在哪\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    # 初始化一个缓冲区用于写入\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "\n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        # 加载当前的块指针\n",
    "        # 由于 ROWS_TILE_SIZE 可能无法整除 ROWS, 且 D_TILE_SIZE 可能无法整除 D\n",
    "        # 因此需要对两个维度进行边界检查\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\") # (ROWS_TILE_SIZE, D_TILE_SIZE)\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\") # (D_TILE_SIZE,)\n",
    "\n",
    "        # 计算行的加权和\n",
    "        output += tl.sum(row * weight[None, :], axis=1)\n",
    "\n",
    "        # 将指针移动到下一个分块\n",
    "        # 这些都是 (rows, columns) 坐标的增量\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))  # # 在最后一个维度移动 D_TILE_SIZE\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))\n",
    "\n",
    "    # 将输出写入输出块指针 (每行一个标量)\n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d7bfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def weighted_sum_backward(\n",
    "    x_ptr, weight_ptr,\n",
    "    grad_output_ptr,  # 梯度输入\n",
    "    grad_x_ptr, partial_grad_weight_ptr,  # 梯度输出\n",
    "    stride_xr, stride_xd,\n",
    "    stride_wd,\n",
    "    stride_gr,\n",
    "    stride_gxr, stride_gxd,\n",
    "    stride_gwb, stride_gwd,\n",
    "    NUM_ROWS, D,\n",
    "    ROWS_TILE_SIZE: tl.constexpr, D_TILE_SIZE: tl.constexpr,\n",
    "):\n",
    "    row_tile_idx = tl.program_id(0)\n",
    "    n_row_tiles = tl.num_programs(0)\n",
    "\n",
    "    # 输入块指针定义\n",
    "    grad_output_block_ptr = tl.make_block_ptr(\n",
    "        grad_output_ptr,\n",
    "        shape=(NUM_ROWS,), strides=(stride_gr),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(NUM_ROWS, D, ), strides=(stride_xr, stride_xd),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,), strides=(stride_wd,),\n",
    "        offsets=(0,), block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    grad_x_block_ptr = tl.make_block_ptr(\n",
    "        grad_x_ptr,\n",
    "        shape=(NUM_ROWS, D,), strides=(stride_gxr, stride_gxd),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_size=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    partial_grad_weight_block_ptr = tl.make_block_ptr(\n",
    "        partial_grad_weight_ptr,\n",
    "        shape=(n_row_tiles, D,), strides=(stride_gwb, stride_gwd),\n",
    "        offsets=(row_tile_idx, 0),\n",
    "        block_shape=(1, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    for i in range(tl.cdiv(D, D_TILE_SIZE)):\n",
    "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
    "\n",
    "        # 计算 grad_x 的外积\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option=\"zero\")\n",
    "        grad_x_row = grad_output[:, None] * weight[None, :]\n",
    "        tl.store(grad_x_block_ptr, grad_x_row, boundary_check=(0, 1))\n",
    "\n",
    "        # 为 grad_wight 结果尽可能多行并行\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        grad_weight_row = tl.sum(row * grad_output[:, None], axis=0, keep_dims=True)\n",
    "        tl.store(partial_grad_weight_block_ptr, grad_weight_row, boundary_check=(1,))\n",
    "\n",
    "        # 沿着 D 移动指针到下一个分块\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))\n",
    "        partial_grad_weight_block_ptr = partial_grad_weight_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        grad_x_block_ptr = grad_x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09707e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.stride(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc3aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Function: 基类, 手动定义一个算子的前向传播 (Forward) 和反向传播 (Backward) 逻辑\n",
    "    一个自定义的 Function 必须继承这个类，并实现两个静态方法 (@staticmethod) forward 和 backward\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        D, output_dims = x.shape[-1], x.shape[:-1]\n",
    "\n",
    "        # 将输入张量重塑(Reshape) 为 2D\n",
    "        input_shape = x.shape\n",
    "        x = rearrange(x, \"... d -> (...) d\")\n",
    "        # 缓存 x 和 weight 以便于反向传播中使用，届时我们只会接收到关于输出张量的梯度，而需要计算关于 x 和 weight 的梯度\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) == 1 and weight.shape[0] == D, \"Dimension mismatch\"\n",
    "        assert x.is_cuda and weight.is_cuda, \"Excepted CUDA tensors\"\n",
    "        assert x.is_contiguous(), \"Our pointer arithmetic will assume contiguous x\"\n",
    "\n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D) // 16  # 列块大小, 循环次数\n",
    "        ctx.ROWS_TILE_SIZE = 16  # 每个线程块一次处理 16 行批次元素,行块大小, 假设有N=100行, 则需要启动 100 / 16 = 7 个线程块并行进行\n",
    "        ctx.input_shape = input_shape\n",
    "        \n",
    "        y = torch.empty(output_dims, device=x.device)\n",
    "\n",
    "        # 在我们的 1D 网格中启动 n 个实例来运行我们的代码\n",
    "        n_rows = y.numel()  # 总共有多少行,即总任务数\n",
    "        \"\"\"\n",
    "        当我们使用 weighted_sum_fwd[(cdiv(n_rows, ctx.ROWS_TILE_SIZE),)] 调用 Triton 内核时，\n",
    "        我们通过传递元组 (cdiv(n_rows, ctx.ROWS_TILE_SIZE),) 定义了一个所谓的“启动网格 (launch grid)”（线程块的网格）。\n",
    "        然后，我们可以在内核中使用 tl.program_id(0) 访问线程块的索引。\n",
    "        \"\"\"\n",
    "        weighted_sum_fwd[(cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](  # n_rows / ctx.ROWS_TILE_SIZE 个线程并行进行\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE, D_TILE_SIZE=ctx.D_TILE_SIZE,\n",
    "        )\n",
    "        return y.view(input_shape[:-1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        x, weight = ctx.saved_tensors\n",
    "        ROWS_TILE_SIZE, D_TILE_SIZE = ctx.ROWS_TILE_SIZE, ctx.D_TILE_SIZE  \n",
    "        n_rows, D = x.shape\n",
    "\n",
    "        # 让每个线程块先写入一部分缓冲区，然后在该缓冲区上进行归约以获得最终梯度\n",
    "        partial_grad_weight = torch.empty((cdiv(n_rows, ROWS_TILE_SIZE), D), device=x.device, dtype=x.dtype)\n",
    "        grad_x = torch.empty_like(x)\n",
    "\n",
    "        weighted_sum_backward[(cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            grad_out,\n",
    "            grad_x, partial_grad_weight,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            grad_out.stride(0),\n",
    "            grad_x.stride(0), grad_x.stride(1),\n",
    "            partial_grad_weight.stride(0), partial_grad_weight.stride(1),\n",
    "            NUM_ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        grad_weight = partial_grad_weight.sum(axis=0)\n",
    "        return grad_x, grad_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "必须通过 .apply 才能连接自动求导\n",
    "WeightedSumFunc 是一个继承自 torch.autograd.Function 的类。\n",
    "\n",
    "你不能直接实例化它：model = WeightedSumFunc() ❌ (这是错的)\n",
    "\n",
    "你也不能直接调用 forward：y = WeightedSumFunc.forward(ctx, x, w) ❌ (这是错的)\n",
    "\n",
    ".apply 是 PyTorch 内部的一个魔法方法。当你调用 WeightedSumFunc.apply(x, w) 时，PyTorch 会在后台做以下事情：\n",
    "\n",
    "创建 ctx (Context) 对象。作用是在 前向传播 (Forward) 和 反向传播 (Backward)。如 ctx.save_for_backward(x, weight)\n",
    "\n",
    "连接前向传播和反向传播的节点。\n",
    "\n",
    "确保梯度能够流过这个操作。\n",
    "\n",
    "所以，你每次使用这个算子，都必须写 WeightedSumFunc.apply(...)。\n",
    "\"\"\"\n",
    "f_weightedsum = WeightedSumFunc.apply  # 定义别名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99890ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.randn(100, 64, device=device, requires_grad=True)\n",
    "weight = torch.randn(64, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f156811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 运行你的 Triton 算子\n",
    "y = f_weightedsum(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5975506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 11.9563,  -8.3941,  14.7706,   2.6851,   0.5113,  -5.6934,  -7.7887,\n",
       "        -16.7739,   0.2786,   1.6083,  -9.1864,  -3.1714,  -4.2149,  -4.5889,\n",
       "         -1.2386,   5.8556, -10.2032,   9.6943,   9.2802,  -5.1951,   0.4868,\n",
       "         -4.7471,  10.7786,   9.2061,  -2.8172,  10.0051, -11.6117,   9.3835,\n",
       "          6.9188,  18.2391,  -7.1230,   4.0649,   4.8202,   0.4916,  11.2586,\n",
       "          5.6963,  11.7063,   6.4597,   0.4835,   9.6063,   1.0958,   1.1348,\n",
       "        -10.2260,  -9.7152,  -0.5819,  -1.5178,  -4.1269,  17.4520,   5.8099,\n",
       "         -8.6497,  -6.9691,   5.1863,   1.3750,  14.0855,  -2.0366,   6.6039,\n",
       "          9.8486, -16.9976,  20.7936,   8.6135,   8.6118,  14.4096, -11.0249,\n",
       "          5.9540,  -1.7794,  -5.2389,  -9.9471,   4.4918,   4.3935,   7.7661,\n",
       "         -4.2371,   2.1426,   5.7930,   9.2507,  15.6980,   6.6524,   4.1412,\n",
       "         -4.8109,  -1.9191,   3.4117,  -1.3624,  10.2873,  -1.5904,   1.0420,\n",
       "          4.0504,  -4.3510,  10.1120,  -1.0156,  -1.8766,  -5.0315,  -1.4905,\n",
       "         12.5342, -12.3259,   4.3293,  -4.6346,   8.1420,  -1.7849,   3.1369,\n",
       "         -0.4868,   2.5600], device='cuda:0',\n",
       "       grad_fn=<WeightedSumFuncBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5bf45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
