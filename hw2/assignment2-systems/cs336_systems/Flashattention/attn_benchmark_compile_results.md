|    |   seq_len |   d_model | forward_ms_mean   | forward_ms_std   | backward_ms_mean   | backward_ms_std   | fwd_peak_mem_MB   |   batch_size |
|---:|----------:|----------:|:------------------|:-----------------|:-------------------|:------------------|:------------------|-------------:|
|  0 |       256 |        16 | 0.194             | 0.151            | 0.621              | 0.114             | 20.9              |            8 |
|  1 |      1024 |        16 | 0.391             | 0.062            | 0.771              | 0.162             | 82.9              |            8 |
|  2 |      4096 |        16 | 1.638             | 0.049            | 4.565              | 0.092             | 1050.8            |            8 |
|  3 |      8192 |        16 | 6.044             | 0.07             | 17.166             | 0.123             | 4133.2            |            8 |
|  4 |     16384 |        16 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
|  5 |       256 |        32 | 0.354             | 0.109            | 0.825              | 0.2               | 21.5              |            8 |
|  6 |      1024 |        32 | 0.362             | 0.056            | 0.767              | 0.028             | 85.4              |            8 |
|  7 |      4096 |        32 | 2.008             | 0.066            | 5.371              | 0.093             | 1060.8            |            8 |
|  8 |      8192 |        32 | 8.757             | 0.028            | 21.308             | 0.123             | 4153.2            |            8 |
|  9 |     16384 |        32 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
| 10 |       256 |        64 | 0.451             | 0.193            | 0.785              | 0.164             | 22.8              |            8 |
| 11 |      1024 |        64 | 0.633             | 0.033            | 1.081              | 0.098             | 90.4              |            8 |
| 12 |      4096 |        64 | 2.821             | 0.034            | 5.975              | 0.096             | 1080.8            |            8 |
| 13 |      8192 |        64 | 7.732             | 0.098            | 20.933             | 0.192             | 4193.2            |            8 |
| 14 |     16384 |        64 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
| 15 |       256 |       128 | 0.29              | 0.057            | 0.788              | 0.091             | 25.3              |            8 |
| 16 |      1024 |       128 | 0.701             | 0.042            | 1.163              | 0.032             | 100.4             |            8 |
| 17 |      4096 |       128 | 3.79              | 0.024            | 7.795              | 0.191             | 1120.8            |            8 |
| 18 |      8192 |       128 | 12.178            | 0.167            | 28.37              | 0.278             | 4273.2            |            8 |
| 19 |     16384 |       128 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |

## 1. 性能对比分析

### 计算速度：大幅提升

`torch.compile` 通过**算子融合（Operator Fusion）**和生成的 **Triton 内核**极大地减少了内存读写开销。

- **前向传播 (Forward)**：在 `seq_len=8192, d_model=128` 时，耗时从 **21.835ms** 降至 **12.178ms**，速度提升了约 **44%**。
- **反向传播 (Backward)**：提升更为惊人。同样在 `seq_len=8192, d_model=128` 时，耗时从 **50.456ms** 锐减至 **28.37ms**。
- **趋势**：序列越长，编译优化的边际效应越明显。这是因为长序列下，$O(N^2)$ 的计算量在算子融合后减少了频繁读写显存（HBM）的带宽限制。

### 显存占用：几乎持平

观察 `fwd_peak_mem_MB` 列：

- 编译前（8192, 128）：**4273.08 MB**
- 编译后（8192, 128）：**4273.28 MB**
- **分析**：尽管 `torch.compile` 优化了计算逻辑，但它依然需要显式地在显存中生成并保留那个形状为 $(8, 8192, 8192)$ 的注意力评分矩阵。因为**算法本身的数学实现没有改变**（依然是朴素 $O(N^2)$），所以显存压力没有得到根本缓解。

### OOM 临界点：没有改变

- 无论是否编译，所有配置在 `seq_len=16384` 时依然全部 **OOM**。
- 这证明了 JIT 编译无法突破 $O(N^2)$ 空间复杂度的物理限制。

------

## 2. 实验结论 

> **结论提炼：**
>
> 虽然 `torch.compile` 能通过融合 Kernel 显著加快运算速度（尤其在长序列下反向传播加速近一倍），但它并未改变 Attention 的显存占用模式。
>
> **根本原因：**
>
> 朴素 Attention 需要存储 $O(Seq^2)$ 的中间激活值（Attention Scores 和 Softmax 概率矩阵）以供反向传播使用。即使经过编译，这些矩阵的物理大小是不变的。在 `seq_len=16384` 时，仅这部分激活值就超出了 GPU 显存限制。
>
> **下一步改进：**
>
> 这正体现了引入 **FlashAttention-2** 的必要性。与 `torch.compile` 这种自动优化不同，FlashAttention 从算法底层重构了计算流程，通过 **Tiling（分块）** 和 **Recomputation（重算）** 彻底消除了对 $O(N^2)$ 中间矩阵的显式存储，从而能够支持 16384 甚至更长的序列。