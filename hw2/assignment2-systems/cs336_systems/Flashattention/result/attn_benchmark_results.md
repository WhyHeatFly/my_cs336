|    |   seq_len |   d_model | forward_ms_mean   | forward_ms_std   | backward_ms_mean   | backward_ms_std   | fwd_peak_mem_MB   |   batch_size |
|---:|----------:|----------:|:------------------|:-----------------|:-------------------|:------------------|:------------------|-------------:|
|  0 |       256 |        16 | 0.339             | 0.084            | 0.995              | 0.201             | 20.9              |            8 |
|  1 |      1024 |        16 | 0.453             | 0.113            | 1.148              | 0.024             | 82.8              |            8 |
|  2 |      4096 |        16 | 4.38              | 0.177            | 10.791             | 0.136             | 1050.6            |            8 |
|  3 |      8192 |        16 | 16.884            | 0.042            | 42.087             | 0.127             | 4133.0            |            8 |
|  4 |     16384 |        16 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
|  5 |       256 |        32 | 0.345             | 0.06             | 1.172              | 0.1               | 21.5              |            8 |
|  6 |      1024 |        32 | 0.428             | 0.064            | 1.213              | 0.16              | 85.3              |            8 |
|  7 |      4096 |        32 | 4.393             | 0.047            | 10.811             | 0.08              | 1060.6            |            8 |
|  8 |      8192 |        32 | 17.155            | 0.027            | 42.334             | 0.148             | 4153.0            |            8 |
|  9 |     16384 |        32 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
| 10 |       256 |        64 | 0.124             | 0.01             | 0.832              | 0.294             | 22.8              |            8 |
| 11 |      1024 |        64 | 0.36              | 0.059            | 1.161              | 0.082             | 90.3              |            8 |
| 12 |      4096 |        64 | 4.556             | 0.027            | 11.187             | 0.16              | 1080.6            |            8 |
| 13 |      8192 |        64 | 17.831            | 0.029            | 43.351             | 0.139             | 4193.0            |            8 |
| 14 |     16384 |        64 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |
| 15 |       256 |       128 | 0.199             | 0.022            | 1.182              | 0.242             | 25.3              |            8 |
| 16 |      1024 |       128 | 0.446             | 0.115            | 1.191              | 0.182             | 100.3             |            8 |
| 17 |      4096 |       128 | 5.537             | 0.051            | 12.936             | 0.092             | 1120.6            |            8 |
| 18 |      8192 |       128 | 21.835            | 0.077            | 50.456             | 0.199             | 4273.0            |            8 |
| 19 |     16384 |       128 | OOM               | OOM              | OOM                | OOM               | OOM               |            8 |

## 1. 显存占用推导 (Memory Accounting)

我们选择表格中**最小的 OOM 配置**：`batch_size=8`, `d_model=16`, `seq_len=16384`。

### 核心开销：注意力评分矩阵 (Attention Score Matrix)

在朴素实现中，最占内存的是 $Q K^T$ 产生的中间矩阵 $S$。

- **矩阵形状**：$(Batch, Seq, Seq) = 8 \times 16384 \times 16384$。

- **元素数量**：$8 \times 268,435,456 = 2,147,483,648$ 个元素。

- **显存占用 (float32)**：每个元素 4 bytes。

  $$2,147,483,648 \times 4 \text{ bytes} \div 1024^3 \approx 8.0 \text{ GB}$$

### 综合计算：

除了评分矩阵，还有：

- **Q, K, V 矩阵**：$3 \times (8 \times 16384 \times 16)$。
- **Softmax 输出**：同样是 $8 \times 16384 \times 16384$ (又一个 8GB)。
- **反向传播梯度**：通常需要保留前向传播的激活值。

**结论**：单单一个评分矩阵和它的 Softmax 结果就吃掉了至少 **16GB** 显存。这解释了为什么在 16384 规模下，普通的消费级 GPU 或分配的算力资源会立即报 OOM。

------

## 2. 交付回答 (1-2 Paragraph Response)

****

根据实验数据，显存溢出（OOM）始终发生在序列长度达到 16384 时。通过内存核算可以发现，内存瓶颈主要源于存储形状为 $(Batch, Seq, Seq)$ 的注意力评分矩阵。在 `seq_len=16384` 且 `batch_size=8` 的配置下，仅这一个中间矩阵在 float32 精度下就会占据约 8GB 的显存，且为了反向传播通常还需保存经过 Softmax 后的概率矩阵，这使得显存需求随序列长度呈二次方 ($O(N^2)$) 增长。随着序列长度从 4096 翻倍至 8192，前向传播的峰值内存从约 1GB 激增至约 4GB，这种增长趋势在处理长文本任务时是不可持续的。

为了消除这一内存成本，可以采用 **FlashAttention-2** 算法。该算法利用了**算子融合（Operator Fusion）**和**分块计算（Tiling）**技术。它不再一次性显式生成并存储整个巨大的 $N \times N$ 注意力评分矩阵，而是通过 **Online Softmax** 在 SRAM 中分块计算注意力输出。这不仅将空间复杂度从 $O(N^2)$ 降低到了 $O(N)$（仅需存储最终输出和少量统计量），还通过减少对高带宽内存（HBM）的读写操作，显著提升了计算速度。