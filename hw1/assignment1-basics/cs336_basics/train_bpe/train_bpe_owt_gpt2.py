import os
from typing import List, Tuple, Dict
import regex as re
from collections import Counter, defaultdict
import time
from tqdm import tqdm
import json
import math
import multiprocessing  # å¤šè¿›ç¨‹æ”¯æŒ
import mmap  # ç”¨äºå†…å­˜æ˜ å°„æ–‡ä»¶

# æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œå‚è€ƒGPT-2çš„åˆ†è¯è§„åˆ™
PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

def gpt2_bytes_to_unicode() -> Dict[int, str]: # 0-255å•å­—èŠ‚æ˜ å°„åˆ°å¯ç¼–ç çš„unicodeå­—ç¬¦
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    This modifies UTF-8 to make sure that all bytes are mapped to
    characters that are valid in UTF-8.
    """
    bs = (
        list(range(ord("!"), ord("~") + 1))
        + list(range(ord("Â¡"), ord("Â¬") + 1))
        + list(range(ord("Â®"), ord("Ã¿") + 1))
    )
    cs = bs[:]
    n = 0
    for b in range(256):
        if b not in bs:
            bs.append(b)
            cs.append(256 + n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))

_BYTES_TO_UNICODE = gpt2_bytes_to_unicode()
_UNICODE_TO_BYTES = {v : k for k, v in _BYTES_TO_UNICODE.items()}

def bytes_to_token_str(b: bytes) -> str:
    # æŠŠä»»æ„bytesè½¬æˆ"remap unicode"ä¸²
    return "".join(_BYTES_TO_UNICODE[x] for x in b)

def token_str_to_bytes(s: str) -> bytes:
    # æŠŠ"remap unicode"ä¸²è½¬å›bytes
    return bytes([_UNICODE_TO_BYTES[c] for c in s])

def _init_vocab(special_tokens: List[str]) -> Dict[int, bytes]:
    """
    _init_vocab: åˆå§‹åŒ–vocabå­—å…¸, åŒ…å«special tokenså’Œ0-255çš„å•å­—èŠ‚å­—ç¬¦
    
    :param special_tokens: ç‰¹æ®Šå­—èŠ‚åˆ—è¡¨ï¼Œå¦‚["<|endoftext|>"]
    :type special_tokens: List[str]
    :return: åˆå§‹åŒ–åçš„vocabå­—å…¸, é”®ä¸ºç´¢å¼•, å€¼ä¸ºå¯¹åº”çš„bytesç±»å‹token
    :rtype: Dict[int, bytes]
    """
    vocab = {}
    idx = 0
    for s in special_tokens:
        vocab[idx] = s.encode("utf-8")  # å­˜ä¸ºbytesç±»å‹
        idx += 1
    
    for i in range(256):
        b = bytes([i])
        vocab[idx] = b
        idx += 1
    
    return vocab

def pre_tokenization(
    text: str,
    special_tokens: List[str]
) -> List[str]:
    """
    pre_tokenization: å°†è®­ç»ƒæ–‡æœ¬è¿›è¡Œé¢„åˆ†è¯
    é¦–å…ˆæŒ‰ç…§ç‰¹æ®Štokenè¿›è¡Œåˆ†å‰²å¹¶ä¿ç•™ç‰¹æ®Štoken, ç„¶åå¯¹éç‰¹æ®Štokenéƒ¨åˆ†æŒ‰ç…§gpt2çš„åˆ†å‰²è§„åˆ™è¿›è¡Œåˆ†è¯
    
    :param text: è®­ç»ƒæ–‡æœ¬
    :type text: str
    :param special_tokens: ç‰¹æ®Šå­—èŠ‚åˆ—è¡¨ï¼Œå¦‚["<|endoftext|>"]
    :type special_tokens: List[str]
    :return: ç»è¿‡é¢„åˆ†è¯åçš„æ–‡æœ¬åˆ—è¡¨ï¼Œå¦‚["This", " is", " a", "<|endoftext|>", " test", "."]
    :rtype: List[str]
    """
    if not special_tokens:
        return re.findall(PAT, text)

    tokens = sorted(special_tokens, key=len, reverse=True)  # æŒ‰é•¿åº¦é™åºæ’åº
    union = "|".join(re.escape(token) for token in tokens)  # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼, ç”¨äºåŒ¹é…ç‰¹æ®Štoken, re.escape()è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
    parts = re.split(f"({union})", text)

    out = []
    st = set(special_tokens)  # ç”¨äºå¿«é€ŸæŸ¥æ‰¾ç‰¹æ®Štoken
    for part in parts:
        if not part:
            continue
        if part in st:
            out.append(part)  # ä¿ç•™ç‰¹æ®Štoken
        else:
            out.extend(re.findall(PAT, part))  # å¯¹éç‰¹æ®Štokenéƒ¨åˆ†è¿›è¡Œgpt2åˆ†è¯
    
    return out

def word_2_byte(word: str) -> Tuple[bytes, ...]:
    """
    word_2_byte: å°†é¢„åˆ†è¯å¾—åˆ°çš„å•è¯è½¬æ¢ä¸ºå¯¹åº”çš„bytesåºåˆ—
    
    :param word: å•è¯å­—ç¬¦ä¸²
    :type word: str
    :return: å¯¹åº”çš„bytesåºåˆ—å…ƒç»„, å¦‚"hello" -> (b'h', b'e', b'l', b'l', b'o')
    :rtype: Tuple[bytes, ...]
    """
    b = word.encode('utf-8')
    return tuple(bytes([x]) for x in b)

def _pairs_in_seq(
    seq: tuple[bytes, ...],
    special_bytes_set: set[bytes]
):
    """
    _pairs_in_seq: ç”Ÿæˆåºåˆ—ä¸­æ‰€æœ‰éç‰¹æ®Štokençš„byteå¯¹
    
    :param seq: å­—èŠ‚åºåˆ—, ä¾‹å¦‚: (b'h', b'e', b'l', b'l', b'o')
    :type seq: tuple[bytes, ...]
    :param special_bytes_set: ç‰¹æ®Šå­—èŠ‚çš„bytesé›†åˆ
    :type special_bytes_set: set[bytes]
    :return: è¯´æ˜
    :rtype: Tuple[bytes, bytes]
    """
    for a, b in zip(seq[:-1], seq[1:]):
        if a in special_bytes_set or b in special_bytes_set:
            continue
        yield (a, b)

def _merge_seq(
    old_word: tuple[bytes, ...],
    best: Tuple[bytes, bytes],
    merged: bytes
) -> tuple[bytes, ...]:
    """
    _merge_seq: å°†æ—§çš„å­—èŠ‚åºåˆ—ä¸­æ‰€æœ‰å‡ºç°çš„bestå¯¹åˆå¹¶ä¸ºmerged
    
    :param old_word: æ—§çš„å­—èŠ‚åºåˆ—, åŒ…å«è¦åˆå¹¶çš„byteå¯¹
    :type old_word: tuple[bytes, ...]
    :param best: è¦åˆå¹¶çš„byteå¯¹
    :type best: Tuple[bytes, bytes]
    :param merged: åˆå¹¶åçš„byte
    :type merged: bytes
    :return: åˆå¹¶åçš„å­—èŠ‚åºåˆ—
    :rtype: tuple[bytes, ...]
    """
    a, b = best
    out = []
    i, n = 0, len(old_word)
    while i < n:
        if i < n - 1 and old_word[i] == a and old_word[i + 1] == b:
            out.append(merged)
            i += 2
        else:
            out.append(old_word[i])
            i += 1
    
    return tuple(out)
# ------------------
# å¹¶è¡Œï¼šworker ä¾§åšâ€œé¢„åˆ†è¯+Counterâ€
# ------------------

_worker_special_tokens = None  # è¿›ç¨‹å†…çš„å…¨å±€å˜é‡ï¼Œworker è¿›ç¨‹åˆå§‹åŒ–æ—¶èµ‹å€¼

def _init_worker(special_tokens: List[str]):
    global _worker_special_tokens
    _worker_special_tokens = special_tokens

# ------------------
# mmap åˆ†å—è¯»å–ï¼šä¿è¯ä¸ä¼šåˆ‡æ–­ utf-8 å­—ç¬¦
# ------------------
def _iter_text_chunks_mmap(
    file_path: str | os.PathLike,
    chunk_bytes: int = 32 * 1024 * 1024
):
    """
    mmap æŒ‰å—è¿­ä»£æ–‡æœ¬
    - ä¸æŠŠ\r\nç­‰æ¢è¡Œç¬¦åˆ‡æ–­
    - æŠŠ\r\n å’Œ \r ç»Ÿä¸€æˆ \n
    """
    # "rb" æ¨¡å¼è¯»å–äºŒè¿›åˆ¶ï¼Œé¿å…æ¢è¡Œç¬¦è½¬æ¢ï¼Œè¯»å‡ºæ¥æ˜¯ bytes
    with open(file_path, "rb") as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            pos, n = 0, len(mm)
            carry = b""  # ä¸Šä¸€å—çš„æ®‹ç•™éƒ¨åˆ†

            while pos < n:
                end = min(pos + chunk_bytes, n)

                # é¿å…æŠŠ \r\n æ‹†æˆä¸¤å—ï¼ˆchunk1 æœ«å°¾æ˜¯ \rï¼Œchunk2 å¼€å¤´æ˜¯ \nï¼‰
                # å› ä¸ºWindowsçš„æ¢è¡Œæ˜¯\r\n
                if end < n and end > 0 and mm[end - 1] == 13 and mm[end] == 10: # 13=\r, 10=\n
                    end += 1 
                
                raw = carry + mm[pos:end]
                pos = end
                # å¤„ç† utf-8 æˆªæ–­: æŠŠä¸å®Œæ•´å­—èŠ‚ç•™åˆ°ä¸‹ä¸€å—
                # å› ä¸º utf-8 æ˜¯å˜é•¿ç¼–ç ï¼Œå¯èƒ½ä¸€ä¸ªå­—ç¬¦å 1-4ä¸ªå­—èŠ‚ï¼Œå¦‚"ä¸­"æ˜¯3ä¸ªå­—èŠ‚ï¼Œå¯èƒ½ä¼šåœ¨ä¸­é—´åˆ‡æ–­
                try:
                    s = raw.decode("utf-8")
                    carry = b""
                except UnicodeDecodeError as e:
                    valid = raw[:e.start]
                    carry = raw[e.start:]
                    s = valid.decode("utf-8", errors="ignore")
                
                if not s:
                    continue

                # å¤ç°æ–‡æœ¬æ¨¡å¼ï¼šç»Ÿä¸€æ¢è¡Œ
                s = s.replace("\r\n", "\n").replace("\r", "\n")
                # ç»Ÿä¸€å¼‚å¸¸æ¢è¡Œç¬¦ï¼ˆLS/PSï¼‰ä¸ºæ™®é€šæ¢è¡Œ
                s = s.replace("\u2028", "\n").replace("\u2029", "\n")

                yield s
            
            # å¤„ç†æœ€åçš„æ®‹ç•™éƒ¨åˆ†
            if carry:
                s = carry.decode("utf-8", errors="ignore")
                s = s.replace("\r\n", "\n").replace("\r", "\n")
                s = s.replace("\u2028", "\n").replace("\u2029", "\n")
                if s:
                    yield s

def _count_chunk_worker(text_chunk: str) -> Counter:
    """
    å¯¹ä¸€ä¸ªæ–‡æœ¬å—åšï¼š
    1) pre_tokenization
    2) ç»Ÿè®¡ seq_counter (è§„åˆ™: special token -> (bytes,) ; æ™®é€š -> tuple(bytes))
    è¿”å›å±€éƒ¨ Counter
    """
    st = _worker_special_tokens
    toks = pre_tokenization(text_chunk, st)
    c = Counter()
    st_set = set(st)
    for w in toks:
        if w in st_set:
            c[(w.encode("utf-8"),)] += 1
        else:
            c[word_2_byte(w)] += 1
    return c

def run_train_bpe(
    input_path: str | os.PathLike,
    vocab_size: int,
    special_tokens: List[str],
    num_processes: int = 8,
    chunk_bytes: int = 32 * 1024 * 1024,  # åˆ†å—å¤§å°ï¼Œé»˜è®¤32MB
    pool_chunksize: int = 4,  # ç»™è¿›ç¨‹æ± çš„chunksize
    **kwargs,
) -> Tuple[Dict[int, str], List[Tuple[bytes, bytes]]]:
    vocab = _init_vocab(special_tokens)

    # ç‰¹æ®Šå­—èŠ‚çš„bytesé›†åˆ, ç”¨æ¥åœ¨åç»­å¤„ç†ä¸­åˆ¤æ–­æ˜¯å¦ä¸ºç‰¹æ®Štoken
    special_token_bytes_set = {s.encode('utf-8') for s in special_tokens}

    # ---------- 1) mmap + å¤šè¿›ç¨‹å¹¶è¡Œç»Ÿè®¡seq_counter ----------
    # ä¸æŠŠæ•´ä¸ªæ–‡ä»¶è¯»åˆ°å†…å­˜ï¼Œè€Œæ˜¯è¿­ä»£chunk
    # æ¯ä¸ª chunk å‘ç»™ worker å¾—åˆ°å±€éƒ¨ Counter, ç„¶åä¸»è¿›ç¨‹ç´¯åŠ 

    seq_counter = Counter()  # ç»Ÿè®¡æ‰€æœ‰seq_byteå‡ºç°çš„é¢‘ç‡

    # é¢„ä¼° chunk æ•°ç”¨äºè¿›åº¦æ¡æ˜¾ç¤º
    file_size = os.path.getsize(input_path)  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    est_total = max(1, math.ceil(file_size / chunk_bytes))

    # æ˜¾å¼é€‰æ‹©å¯åŠ¨å­è¿›ç¨‹çš„æ–¹å¼ï¼Œå…¼å®¹Windows
    ctx = multiprocessing.get_context("spawn")  # ä»¥"spawn"æ–¹å¼å¯åŠ¨å­è¿›ç¨‹

    with ctx.Pool(
        processes=num_processes,
        initializer=_init_worker,
        initargs=(special_tokens,),
    ) as pool:
        chunk_iter = _iter_text_chunks_mmap(input_path, chunk_bytes=chunk_bytes)

        # imap è¾¹äº§ç”Ÿè¾¹æ¶ˆè´¹
        for local_counter in tqdm(
            pool.imap(_count_chunk_worker, chunk_iter, chunksize=pool_chunksize),
            total=est_total,
            desc="Counting (pre-tokenize + freq)",
            unit="chunk",
            mininterval=0.5,
        ):
            seq_counter.update(local_counter)

    seqs = list(seq_counter.keys())  # seqsæ˜¯æ‰€æœ‰ä¸åŒçš„byteåºåˆ—, ä¾‹å¦‚: [(b'h', b'e', b'l', b'l', b'o'), (b'w', b'o', b'r', b'l', b'd')]
    freqs = [seq_counter[s] for s in seqs]  # freqsæ˜¯å¯¹åº”çš„é¢‘æ¬¡åˆ—è¡¨

    # --- åˆå§‹åŒ–pair_cnt ---
    pair_cnt = Counter() # ç»Ÿè®¡æ‰€æœ‰byteå¯¹å‡ºç°çš„é¢‘æ¬¡
    pair_to_words = defaultdict(set)  # è®°å½•æ¯ä¸ªbyteå¯¹å‡ºç°åœ¨å“ªäº›byteåºåˆ—ä¸­

    for wi, seq in enumerate(seqs):
        for p in _pairs_in_seq(seq, special_token_bytes_set):
            pair_cnt[p] += freqs[wi]
            pair_to_words[p].add(wi)

    # --- è¿­ä»£åˆå¹¶ BPEè®­ç»ƒ ---
    merges: List[Tuple[bytes, bytes]] = []

    def pick_best_pair() -> Tuple[bytes, bytes]: 
        # å…ˆæŒ‰ç…§é¢‘æ¬¡æœ€å¤§çš„é€‰ï¼Œå†æŒ‰ç…§å­—å…¸åºæœ€å¤§çš„é€‰
        return max(pair_cnt.items(), key=lambda kv: (kv[1], kv[0]))[0]
    
    target_merges = vocab_size - len(vocab)
    pbar = tqdm(total=max(0, target_merges), desc="Training BPE merges", unit="merge", mininterval=0.5)

    while len(vocab) < vocab_size and pair_cnt:
        best = pick_best_pair()
        a, b = best
        merged = a + b

        merges.append(best)
        vocab[len(vocab)] = merged

        affected_word_indices = list(pair_to_words.get(best, ()))  # è·å–æ‰€æœ‰åŒ…å«è¯¥pairçš„wordç´¢å¼•

        if not affected_word_indices:
            pair_cnt.pop(best, None)
            pair_to_words.pop(best, None)
            pbar.update(1)
            continue

        # æ›´æ–°æ‰€æœ‰å—å½±å“çš„word
        for wi in affected_word_indices:
            old_word = seqs[wi]
            word_freq = freqs[wi]

            # ç§»é™¤æ—§pairè®¡æ•°
            for p in _pairs_in_seq(old_word, special_token_bytes_set):
                pair_cnt[p] -= word_freq
                if pair_cnt[p] <= 0:
                    pair_cnt.pop(p, None)
                pair_to_words[p].discard(wi)
            
            # merge
            new = _merge_seq(old_word, best, merged)
            seqs[wi] = new

            # æ·»åŠ æ–°pairè®¡æ•°
            for p in _pairs_in_seq(new, special_token_bytes_set):
                pair_cnt[p] += word_freq
                pair_to_words[p].add(wi)
        
        # ç§»é™¤å·²åˆå¹¶çš„pair
        pair_to_words.pop(best, None)
        pair_cnt.pop(best, None)
        pbar.update(1)
    pbar.close()
    
    return vocab, merges


def save_vocab_and_merges(
    vocab: Dict[int, bytes],
    merges: List[Tuple[bytes, bytes]],
    vocab_path: str | os.PathLike,
    merges_path: str | os.PathLike
):
    # 1. ä¿å­˜è¯æ±‡è¡¨(jsonæ ¼å¼)
    vocab_str = {idx: bytes_to_token_str(token) for idx, token in vocab.items()}
    with open(vocab_path, "w", encoding="utf-8") as f:
        json.dump(vocab_str, f, ensure_ascii=False, indent=2)
    
    # 2. ä¿å­˜åˆå¹¶è§„åˆ™(æ–‡æœ¬æ ¼å¼)
    with open(merges_path, "w", encoding="utf-8") as f:
        for a, b in merges:
            part1 = bytes_to_token_str(a)
            part2 = bytes_to_token_str(b)
            f.write(f"{part1} {part2}\n")
    
if __name__ == "__main__":

    train_path = "../../../../data/owt_train.txt/owt_train.txt"
    vocab_size = 32000
    special_tokens = ["<|endoftext|>"]

    if not os.path.exists(train_path):
        raise FileNotFoundError(f"File not found: {train_path}")
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    start_time = time.time()
    vocab, merges = run_train_bpe(train_path, vocab_size, special_tokens)
    print(f"âœ… è®­ç»ƒå®Œæˆ, ç”¨æ—¶ {time.time() - start_time:.2f} ç§’,  {(time.time() - start_time)/60:.2f} åˆ†é’Ÿ")

    # ä¿å­˜ç»“æœ
    output_dir = "./bpe_output"
    os.makedirs(output_dir, exist_ok=True)

    vocab_path = os.path.join(output_dir, "vocab_on_owt_gpt2.json")
    merges_path = os.path.join(output_dir, "merges_on_owt_gpt2.txt")
    save_vocab_and_merges(vocab, merges, vocab_path, merges_path)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {output_dir}")



