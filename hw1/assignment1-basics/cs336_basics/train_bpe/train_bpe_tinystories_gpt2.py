import os
import math
import mmap
import regex as re
import multiprocessing as mp
from typing import List, Dict, Tuple, Iterable
from collections import Counter, defaultdict
from tqdm import tqdm
import time
import json

PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""

def gpt2_bytes_to_unicode() -> Dict[int, str]: # 0-255å•å­—èŠ‚æ˜ å°„åˆ°å¯ç¼–ç çš„unicodeå­—ç¬¦
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    This modifies UTF-8 to make sure that all bytes are mapped to
    characters that are valid in UTF-8.
    """
    bs = (
        list(range(ord("!"), ord("~") + 1))
        + list(range(ord("Â¡"), ord("Â¬") + 1))
        + list(range(ord("Â®"), ord("Ã¿") + 1))
    )
    cs = bs[:]
    n = 0
    for b in range(256):
        if b not in bs:
            bs.append(b)
            cs.append(256 + n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))

_BYTES_TO_UNICODE = gpt2_bytes_to_unicode()
_UNICODE_TO_BYTES = {v : k for k, v in _BYTES_TO_UNICODE.items()}

def bytes_to_token_str(b: bytes) -> str:
    # æŠŠä»»æ„bytesè½¬æˆ"remap unicode"ä¸²
    return "".join(_BYTES_TO_UNICODE[x] for x in b)

def token_str_to_bytes(s: str) -> bytes:
    # æŠŠ"remap unicode"ä¸²è½¬å›bytes
    return bytes([_UNICODE_TO_BYTES[c] for c in s])

def pre_tokenization(s: str, special_tokens: List[str]) -> list[str]:
    if not special_tokens:
        return re.findall(PAT, s)

    tokens = sorted(special_tokens, key=len, reverse=True)
    union = "|".join(re.escape(token) for token in tokens)
    parts = re.split(f"({union})", s)

    out = []
    st = set(special_tokens)
    for part in parts:
        if not part:
            continue
        if part in st:
            out.append(part)  # special token ä¿ç•™
        else:
            out.extend(re.findall(PAT, part))
    return out

def _init_vocab(special_tokens: List[str]) -> Dict[int, bytes]:
    vocab = {}
    idx = 0
    for s in special_tokens:
        vocab[idx] = s.encode("utf-8")
        idx += 1
    for i in range(256):
        vocab[idx] = bytes([i])
        idx += 1
    return vocab

def word_2_byte(word: str) -> Tuple[bytes, ...]:
    b = word.encode("utf-8")
    return tuple(bytes([x]) for x in b)

def _pairs_in_seq(seq: tuple[bytes, ...], special_bytes_set: set[bytes]):
    for a, b in zip(seq[:-1], seq[1:]):
        if a in special_bytes_set or b in special_bytes_set:
            continue
        yield (a, b)

def _merge_seq(seq: tuple[bytes, ...], pair: Tuple[bytes, bytes], merged: bytes):
    a, b = pair
    out = []
    i, n = 0, len(seq)
    while i < n:
        if i < n - 1 and seq[i] == a and seq[i + 1] == b:
            out.append(merged)
            i += 2
        else:
            out.append(seq[i])
            i += 1
    return tuple(out)

# ---------------------------
# å¹¶è¡Œï¼šworker ä¾§åšâ€œé¢„åˆ†è¯+Counterâ€
# ---------------------------
_worker_special_tokens = None

def _init_worker(special_tokens: List[str]):
    # Windows spawn ä¸‹éœ€è¦ initializer ç»™å…¨å±€å˜é‡èµ‹å€¼
    global _worker_special_tokens
    _worker_special_tokens = special_tokens

def _count_chunk_worker(text_chunk: str) -> Counter:
    """
    å¯¹ä¸€ä¸ªæ–‡æœ¬å—åšï¼š
    1) pre_tokenization
    2) ç»Ÿè®¡ seq_counterï¼ˆè§„åˆ™ï¼šspecial token -> (bytes,) ; æ™®é€š -> tuple(bytes)ï¼‰
    è¿”å›å±€éƒ¨ Counter
    """
    st = _worker_special_tokens
    toks = pre_tokenization(text_chunk, st)
    c = Counter()
    st_set = set(st)
    for w in toks:
        if w in st_set:
            c[(w.encode("utf-8"),)] += 1
        else:
            c[word_2_byte(w)] += 1
    return c

# ---------------------------
# mmap åˆ†å—è¯»å–ï¼šä¿è¯ä¸ä¼šåˆ‡æ–­ utf-8
# ---------------------------
def _iter_text_chunks_mmap(
    file_path: str | os.PathLike,
    chunk_bytes: int = 32 * 1024 * 1024,  # 32MB
):
    """
    mmap æŒ‰å—è¿­ä»£æ–‡æœ¬ï¼Œå¹¶å°½é‡å¤ç° Python æ–‡æœ¬æ¨¡å¼çš„ universal newline è¡Œä¸ºï¼š
    - ä¸æŠŠ \r\n åˆ‡æ–­
    - æŠŠ \r\n å’Œ \r ç»Ÿä¸€æˆ \n
    """
    with open(file_path, "rb") as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            n = len(mm)
            pos = 0
            carry = b""

            while pos < n:
                end = min(pos + chunk_bytes, n)

                # å…³é”®ï¼šé¿å…æŠŠ \r\n æ‹†æˆä¸¤å—ï¼ˆchunk1 æœ«å°¾æ˜¯ \rï¼Œchunk2 å¼€å¤´æ˜¯ \nï¼‰
                if end < n and end > 0 and mm[end - 1] == 13 and mm[end] == 10:  # 13=\r, 10=\n
                    end += 1

                raw = carry + mm[pos:end]
                pos = end

                # å¤„ç† UTF-8 æˆªæ–­ï¼šæŠŠä¸å®Œæ•´å­—èŠ‚ç•™åˆ°ä¸‹ä¸€å—
                try:
                    s = raw.decode("utf-8")
                    carry = b""
                except UnicodeDecodeError as e:
                    valid = raw[:e.start]
                    carry = raw[e.start:]
                    s = valid.decode("utf-8", errors="ignore")

                if not s:
                    continue

                # å¤ç°æ–‡æœ¬æ¨¡å¼ï¼šç»Ÿä¸€æ¢è¡Œ
                s = s.replace("\r\n", "\n").replace("\r", "\n")
                # ç»Ÿä¸€å¼‚å¸¸æ¢è¡Œç¬¦ï¼ˆLS/PSï¼‰
                s = s.replace("\u2028", "\n").replace("\u2029", "\n")

                yield s

            if carry:
                s = carry.decode("utf-8", errors="ignore")
                s = s.replace("\r\n", "\n").replace("\r", "\n")
                s = s.replace("\u2028", "\n").replace("\u2029", "\n")
                if s:
                    yield s

# ---------------------------
# ä¸»å‡½æ•°ï¼šå¹¶è¡Œæ„å»º seq_counter + åŸé€»è¾‘åˆå¹¶
# ---------------------------
def run_train_bpe(
    input_path: str | os.PathLike,
    vocab_size: int,
    special_tokens: List[str],
    num_processes: int = 8,
    chunk_bytes: int = 32 * 1024 * 1024,  # åˆ†å—å¤§å°ï¼Œé»˜è®¤32MB
    pool_chunksize: int = 4,  # ç»™è¿›ç¨‹æ± çš„chunksize
    **kwargs,
) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:

    vocab = _init_vocab(special_tokens)
    special_bytes_set = {s.encode("utf-8") for s in special_tokens}

    # ---------- 1) mmap + å¤šè¿›ç¨‹å¹¶è¡Œç»Ÿè®¡ seq_counter ----------
    # è¯´æ˜ï¼šæˆ‘ä»¬ä¸å†æŠŠå…¨æ–‡ä»¶è¯»å…¥å†…å­˜ï¼Œè€Œæ˜¯è¿­ä»£ chunkï¼›
    # æ¯ä¸ª chunk å‘ç»™ worker å¾—åˆ°å±€éƒ¨ Counterï¼Œç„¶åä¸»è¿›ç¨‹ç´¯åŠ ã€‚
    seq_counter = Counter()

    # é¢„ä¼° chunk æ•°ç”¨äº tqdmï¼ˆä¸ç²¾ç¡®ä¹Ÿæ²¡å…³ç³»ï¼‰
    file_size = os.path.getsize(input_path)  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    est_total = max(1, math.ceil(file_size / chunk_bytes))  # é¢„ä¼°å—æ•°

    # Windows ä¸Šå»ºè®®ä½¿ç”¨ spawnï¼Œå¹¶ä¸”è¦åœ¨ main é‡Œè°ƒç”¨
    ctx = mp.get_context("spawn")  # ä»¥"spawn"æ–¹å¼å¯åŠ¨å­è¿›ç¨‹

    with ctx.Pool(
        processes=num_processes,
        initializer=_init_worker,
        initargs=(special_tokens,),
    ) as pool:
        chunk_iter = _iter_text_chunks_mmap(input_path, chunk_bytes=chunk_bytes)

        # imap å¯ä»¥è¾¹äº§ç”Ÿè¾¹æ¶ˆè´¹ï¼Œå†…å­˜æ›´ç¨³
        for local_counter in tqdm(
            pool.imap(_count_chunk_worker, chunk_iter, chunksize=pool_chunksize),
            total=est_total,
            desc="Counting (pre-tokenize + freq)",
            unit="chunk",
            mininterval=0.5,
        ):
            seq_counter.update(local_counter)

    # ---------- 2) åé¢ä¿æŒä½ åŸé€»è¾‘ï¼šæ„å»º pair_cnt/pair_to_words + åˆå¹¶ ----------
    seqs = list(seq_counter.keys())
    freqs = [seq_counter[s] for s in seqs]

    pair_cnt = Counter()
    pair_to_words = defaultdict(set)

    for wi, seq in enumerate(seqs):
        for p in _pairs_in_seq(seq, special_bytes_set):
            pair_cnt[p] += freqs[wi]
            pair_to_words[p].add(wi)

    merges: List[Tuple[bytes, bytes]] = []

    def pick_best_pair():
        # é¢‘æ¬¡æœ€å¤§ä¼˜å…ˆï¼Œé¢‘æ¬¡ç›¸åŒæŒ‰å­—å…¸åºæœ€å¤§ï¼ˆä¿æŒä½ åŸè§„åˆ™ï¼‰
        return max(pair_cnt.items(), key=lambda kv: (kv[1], kv[0]))[0]

    target_merges = vocab_size - len(vocab)
    pbar = tqdm(total=max(0, target_merges), desc="Training BPE merges", unit="merge", mininterval=0.5)

    while len(vocab) < vocab_size and pair_cnt:
        best = pick_best_pair()
        a, b = best
        merged = a + b

        merges.append(best)
        vocab[len(vocab)] = merged

        affected = list(pair_to_words.get(best, ()))
        if not affected:
            pair_cnt.pop(best, None)
            pair_to_words.pop(best, None)
            pbar.update(1)
            continue

        for wi in affected:
            old_word = seqs[wi]
            word_freq = freqs[wi]

            for p in _pairs_in_seq(old_word, special_bytes_set):
                pair_cnt[p] -= word_freq
                if pair_cnt[p] <= 0:
                    pair_cnt.pop(p, None)
                pair_to_words[p].discard(wi)

            new = _merge_seq(old_word, best, merged)
            seqs[wi] = new

            for p in _pairs_in_seq(new, special_bytes_set):
                pair_cnt[p] += word_freq
                pair_to_words[p].add(wi)

        pair_to_words.pop(best, None)
        pair_cnt.pop(best, None)

        pbar.update(1)

    pbar.close()
    return vocab, merges

def save_vocab_and_merges(
    vocab: Dict[int, bytes],
    merges: List[Tuple[bytes, bytes]],
    vocab_path: str | os.PathLike,
    merges_path: str | os.PathLike
):
    # 1. ä¿å­˜è¯æ±‡è¡¨(jsonæ ¼å¼)
    # æœ‰äº›å­—èŠ‚ä¸æ˜¯åˆæ³•çš„ utf-8 å•å­—èŠ‚ï¼Œå› æ­¤åœ¨ä¿å­˜çš„æ—¶å€™ errors='ignore' ä¼šåæ‰è¿™äº›éæ³•å­—èŠ‚ï¼Œå¯¼è‡´ç¼–ç å¤±è´¥
    # å› æ­¤é‡‡ç”¨gpt2çš„bytes_to_token_stræ–¹æ³•è¿›è¡Œè½¬æ¢ï¼Œè½¬æˆå¯ç¼–ç çš„unicodeå­—ç¬¦ä¸²
    vocab_str = {idx: bytes_to_token_str(token) for idx, token in vocab.items()}
    with open(vocab_path, "w", encoding="utf-8") as f:
        json.dump(vocab_str, f, ensure_ascii=False, indent=2)
    
    # 2. ä¿å­˜åˆå¹¶è§„åˆ™(æ–‡æœ¬æ ¼å¼)
    with open(merges_path, "w", encoding="utf-8") as f:
        for a, b in merges:
            part1 = bytes_to_token_str(a)
            part2 = bytes_to_token_str(b)
            f.write(f"{part1} {part2}\n")

if __name__ == "__main__":

    train_path = "../../../../data/TinyStoriesV2-GPT4-train.txt"
    vocab_size = 10000
    special_tokens = ["<|endoftext|>"]

    if not os.path.exists(train_path):
        raise FileNotFoundError(f"File not found: {train_path}")
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    start_time = time.time()
    vocab, merges = run_train_bpe(train_path, vocab_size, special_tokens)
    print(f"âœ… è®­ç»ƒå®Œæˆ, ç”¨æ—¶ {time.time() - start_time:.2f} ç§’,  {(time.time() - start_time)/60:.2f} åˆ†é’Ÿ")

    # ä¿å­˜ç»“æœ
    output_dir = "./bpe_output"
    os.makedirs(output_dir, exist_ok=True)

    vocab_path = os.path.join(output_dir, "vocab_on_tinystories_gpt2.json")
    merges_path = os.path.join(output_dir, "merges_on_tinystories_gpt2.txt")
    save_vocab_and_merges(vocab, merges, vocab_path, merges_path)
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {output_dir}")
